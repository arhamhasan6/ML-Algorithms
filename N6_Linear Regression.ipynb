{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Running Gradient Descent on Cost Function of Univariate Linear Regression\n",
    "\n",
    "### Mean Squared Error: a cost function for regression problems\n",
    "\n",
    "### $$MSE = \\frac{1}{2n} \\sum_{i=1}^{n} \\big( y^{(i)} - \\theta_0 - \\theta_1.x^{(i)} \\big)^2 $$\n",
    "\n",
    "\n",
    "### Partial Derivatives of MSE w.r.t. $\\theta_0$ and $\\theta_1$\n",
    "\n",
    "## $$\\frac{\\partial MSE}{\\partial \\theta_0} = - \\frac{1}{n} \\sum_{i=1}^{n} \\big( y^{(i)} - \\theta_0 - \\theta_1 x^{(i)} \\big)$$\n",
    "\n",
    "## $$\\frac{\\partial MSE}{\\partial \\theta_1} = - \\frac{1}{n} \\sum_{i=1}^{n} \\big( y^{(i)} - \\theta_0 - \\theta_1 x^{(i)} \\big) \\big( x^{(i)} \\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real data for x and y (dataset with n=7)\n",
    "x_i = [0.1, 1.2, 2.4, 3.2, 4.1, 5.7, 6.5]\n",
    "y_i = [1.7, 2.4, 3.5, 3.0, 6.1, 9.4, 8.2]\n",
    "\n",
    "#Making data for theta_0 and theta_1\n",
    "th_0 = np.linspace(start=-1, stop=3, num=200)\n",
    "th_1 = np.linspace(start=-1, stop=3, num=200)\n",
    "plot_t0, plot_t1 = np.meshgrid(th_0, th_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cost function - MSE\n",
    "def mse(theta_0,theta_1):\n",
    "    total=0\n",
    "    for j in range(len(x_i)):\n",
    "        total+=(y_i[j]-theta_0-theta_1*x_i[j])**2\n",
    "    return total/(2*len(x_i))\n",
    "\n",
    "#Partial derivative wrt theta_0\n",
    "def pdtheta_0(theta_1,theta_0): \n",
    "    total=0\n",
    "    for j in range(len(x_i)):\n",
    "        total+=y_i[j]-theta_0-theta_1*x_i[j]\n",
    "    return total/(-len(x_i))\n",
    "\n",
    "#Partial derivative wrt theta_1\n",
    "def pdtheta_1(theta_1,theta_0):\n",
    "    total=0\n",
    "    for j in range(len(x_i)):\n",
    "        total+=(y_i[j]-theta_0-theta_1*x_i[j])*x_i[j]\n",
    "    return total/(-len(x_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent implementation\n",
    "learning_rate = 0.01\n",
    "max_iter = 200\n",
    "initial_theta_0 = 1\n",
    "initial_theta_1= 1\n",
    "\n",
    "theta_0_old=initial_theta_0\n",
    "theta_1_old=initial_theta_1\n",
    "\n",
    "theta_0_list=[]\n",
    "theta_1_list=[]\n",
    "for i in range (1,max_iter+1):\n",
    "    theta_0_new = theta_0_old - learning_rate * pdtheta_0(theta_0_old,theta_1_old)\n",
    "    theta_1_new = theta_1_old - learning_rate * pdtheta_1(theta_0_old,theta_1_old)\n",
    "    \n",
    "    theta_0_list.append(theta_0_old)\n",
    "    theta_1_list.append(theta_1_old)\n",
    "\n",
    "    theta_0_old=theta_0_new\n",
    "    theta_1_old=theta_1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a 3D graph\n",
    "fig=plt.figure(figsize=[16,12])\n",
    "ax=plt.axes(projection='3d')\n",
    "ax.set_xlabel('theta 0', fontsize=16)\n",
    "ax.set_ylabel('theta 1', fontsize=16)\n",
    "ax.set_zlabel('Cost - MSE', fontsize=16)\n",
    "ax.plot_surface(plot_t0, plot_t1, mse(plot_t0, plot_t1), alpha=0.4, cmap='summer')\n",
    "ax.scatter(theta_0_list,theta_1_list,mse(np.array(theta_0_list),np.array(theta_1_list)), alpha=0.4, s=50, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Making Predictions using the Movie Revenue Dataset\n",
    "### An Example of Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading movie revenue dataset from csv\n",
    "data=pd.read_csv('D1_Movie_Revenue_Dataset_Clean.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the feature into variable X\n",
    "X=data['Production_Budget'] #extracts column as a series\n",
    "type(X)\n",
    "X=data[['Production_Budget']] #extracts column as a dataframe\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the target into variable X\n",
    "Y=data[['Worldwide_Gross']]\n",
    "type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X,Y,alpha=0.3) #alpha sets transparency of dots\n",
    "plt.title('Budget vs Revenue',fontsize=14)\n",
    "plt.xlabel('Movie Budget $',fontsize=14)\n",
    "plt.ylabel('Movie bRvenue $',fontsize=14)\n",
    "plt.xticks(color='red',fontsize=14)\n",
    "plt.yticks(color='red',fontsize=14)\n",
    "plt.xlim(0,450000000)\n",
    "plt.ylim(0,1000000000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Linear Regression Model\n",
    "r=LinearRegression() #Creates an object of LinearRegression type\n",
    "r.fit(X,Y) #Does the training to select optimal values of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading values for parameters theta_0 and theta_1\n",
    "\n",
    "#theta_0\n",
    "t0=r.intercept_[0]  \n",
    "#theta_1\n",
    "t1=r.coef_[0][0]  \n",
    "print('theta_0:',t0)\n",
    "print('theta_1:',t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading R-square value (the goodness of fit)\n",
    "r.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "\n",
    "#Approach 1: Using the equation of linear regression\n",
    "y=t0+t1*5000000\n",
    "print('Prediction 1:',y)\n",
    "\n",
    "#Approach 2: Using the predict method\n",
    "y=r.predict(pd.DataFrame([5000000]))\n",
    "print('Prediction 2:',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions for the whole dataset\n",
    "y=r.predict(X)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the fit line over the scatter plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X,Y,alpha=0.3)\n",
    "plt.title('Budget vs Revenue',fontsize=14)\n",
    "plt.xlabel('Movie Budget $',fontsize=14)\n",
    "plt.ylabel('Movie bRvenue $',fontsize=14)\n",
    "plt.xticks(color='red',fontsize=14)\n",
    "plt.yticks(color='red',fontsize=14)\n",
    "plt.xlim(0,450000000)\n",
    "plt.ylim(0,1000000000)\n",
    "plt.plot(X,r.predict(X),color='red',linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Altenate Model\n",
    "### Applying Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X, Y,test_size=0.2,random_state=0)\n",
    "print('Size of original dataset (complete):',len(X))\n",
    "print('Size of train dataset (80%):',len(X_train))\n",
    "print('Size of test dataset (20%):',len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us now fit the model on train data and then check the r_squared value of both train and test sets\n",
    "r2=LinearRegression()\n",
    "r2.fit(X_train,Y_train)\n",
    "print(r2.score(X_train,Y_train))\n",
    "print(r2.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Making Predictions using the Boston Housing Dataset\n",
    "### An Example of Multivariable Linear Regression\n",
    "\n",
    "Refer to notebook 'N4_Data Preprocessing and Feature Engineeing_4.ipynb' for preliminary details on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset from the csv file\n",
    "boston_dataset = pd.read_csv('Boston_Dataset.csv')\n",
    "boston_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a series for the target\n",
    "target=boston_dataset['PRICE']\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe for the features\n",
    "features=boston_dataset.drop(['PRICE'],axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting sataset into train and test parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK FOR YOU\n",
    "Find sizes of train and test sets and verify if the split is actually 80%-20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running multivariable linear regression\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "print('Training data r-square:', regr.score(X_train, y_train))\n",
    "print('Test data r-square:', regr.score(X_test, y_test))\n",
    "\n",
    "print('Intercept', regr.intercept_)\n",
    "\n",
    "#Regression coefficients for all features\n",
    "pd.DataFrame(data=regr.coef_, index=X_train.columns, columns=['coef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Significance of a Feature using p_values\n",
    "\n",
    "- There is no direct method in sklearn package to find p_values, so we are using another package called statsmodel for this purpose.\n",
    "- We will have to run the model fitting again using this new package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reproducing the same results using statsmodel package\n",
    "X_incl_const = sm.add_constant(X_train)# this is to add an additional row for constant (theta_0)\n",
    "\n",
    "alternate_model = sm.OLS(y_train, X_incl_const)\n",
    "results = alternate_model.fit()\n",
    "\n",
    "#Finding p-values\n",
    "pd.DataFrame({'coef': results.params, 'p-value': round(results.pvalues, 3)})\n",
    "#Note that the coefficent values are same as the first model m1. Row one shows the value for theta_0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We know from theory class, if p-value of a feature is < 0.05 then it is significant, otherwise not.\n",
    "- So we conclude from the above results that INDUS and AGE are the two features which might not be significant to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Finding correlations among variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding correlations between the features and target - tabular form\n",
    "features.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding correlations between the features and target - graphical heatmap\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(features.corr(),  annot=True, annot_kws={\"size\": 14})\n",
    "sns.set_style('white')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude the following high correlations may possibly cause multicollinearity:\n",
    "- between TAX and RAD\n",
    "- between DIS and NOX\n",
    "- between DIS and INDUS\n",
    "- between DIS and AGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Finding variance inflation factors\n",
    "- Variance Inflation Factor (VIF) can also be reviewed for detection of multicollineairy.\n",
    "- This was not discussed in the topic 'Data Preprocessing and Feature Engineering', as its evaluation requires understanding of linear regression.\n",
    "\n",
    "Description:\n",
    "- VIF measures how much of the variation in one variable is explained by the other variable.\n",
    "- This is done by running a regression using one of the correlated features as the dependent variable against the other variables as predictor variables.\n",
    "- The VIF value can be interpreted as follows:\n",
    "   \n",
    "   If VIF = 1, no correlation exists\n",
    "   \n",
    "   If VIF is between 1 and 5, moderate correlation exists\n",
    "   \n",
    "   If VIF is between 5 and 10, high correlation exists\n",
    "   \n",
    "   If VIF value is >10 then it is problematic.\n",
    "   \n",
    "Formula:\n",
    "$$ VIF _{TAX} = \\frac{1}{(1 - R ^ 2)} $$\t\n",
    "\n",
    "- Since it is calculated separately for each feature, the following is the calculation of VIF for TAX \n",
    "$$ TAX = \\alpha _0 + \\alpha _1 RM + \\alpha _2 NOX + ... + \\alpha _{12}LSTAT $$\n",
    "\n",
    "$$ VIF _{TAX} = \\frac{1}{(1 - R _{TAX} ^ 2)} $$\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding VIF for 'TAX'\n",
    "#This method is also from statsmodel package\n",
    "variance_inflation_factor(exog=X_incl_const.values, exog_idx=10) \n",
    "#X_incl_const.values gives ndarray version of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding VIF for all columns\n",
    "vif = [] # empty list\n",
    "for i in range(X_incl_const.shape[1]):\n",
    "    vif.append(variance_inflation_factor(exog=X_incl_const.values, exog_idx=i))\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that the following two values are high and show possible signs of multicollinearity:\n",
    "- 7.314299817005058, corresponds to feature RAD\n",
    "- 8.508856493040817, corresponds to feature TAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing\n",
    "- Possible insignificant features: INDUS and AGE\n",
    "\n",
    "- Features exhibiting signs of multicollinearity: TAX and RAD (as these were indicated by both methods)\n",
    "\n",
    "\n",
    "- Now let's drop these features one by one and see the results.\n",
    "- That means we will create different models with different subset of features, selecting the best model as the final one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1: with all the 13 features\n",
    "m1 = LinearRegression()\n",
    "m1.fit(X_train, y_train)\n",
    "print('With all 13 features:')\n",
    "print('Training data r-squared:', m1.score(X_train, y_train))\n",
    "print('Test data r-squared:', m1.score(X_test, y_test))\n",
    "\n",
    "#Model 2: dropping 'AGE' and 'INDUS'\n",
    "X_train2=X_train.drop(['AGE','INDUS'],axis=1)\n",
    "X_test2=X_test.drop(['AGE','INDUS'],axis=1)\n",
    "m2 = LinearRegression()\n",
    "m2.fit(X_train2, y_train)\n",
    "print('\\nDropping AGE and INDUS:')\n",
    "print('Training data r-squared:', m2.score(X_train2, y_train))\n",
    "print('Test data r-squared:', m2.score(X_test2, y_test))\n",
    "\n",
    "#Model 3: dropping 'RAD' and 'TAX'\n",
    "X_train3=X_train.drop(['RAD','TAX'],axis=1)\n",
    "X_test3=X_test.drop(['RAD','TAX'],axis=1)\n",
    "m3 = LinearRegression()\n",
    "m3.fit(X_train3, y_train)\n",
    "print('\\nDropping RAD and TAX:')\n",
    "print('Training data r-squared:', m3.score(X_train3, y_train))\n",
    "print('Test data r-squared:', m3.score(X_test3, y_test))\n",
    "\n",
    "#Model 4: dropping 'RM'\n",
    "X_train3=X_train.drop(['RM'],axis=1)\n",
    "X_test3=X_test.drop(['RM'],axis=1)\n",
    "m3 = LinearRegression()\n",
    "m3.fit(X_train3, y_train)\n",
    "print('\\nDropping RM:')\n",
    "print('Training data r-squared:', m3.score(X_train3, y_train))\n",
    "print('Test data r-squared:', m3.score(X_test3, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We conclude that dropping AGE and INDUS does not degrade the model, so these features are not effecting the predictions much and can be safely dropped.\n",
    "- Same is the case with RAD and TAX. You can also try dropping all 4.\n",
    "- But dropping RM causes performance to drop significantly, showing that it is a significant feature in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
