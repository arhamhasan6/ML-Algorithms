{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SCALING EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: The Boston House Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "Scikit-learn toy datasets: scikit-learn package of Python comes with a few small standard datasets that do not require to download any file from some external website.\n",
    "https://scikit-learn.org/stable/datasets/toy_dataset.html\n",
    "\n",
    "This Boston House Pricingdata has been taken from a reaserch paper, which can be found at \n",
    "[Source: Original research paper](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/22636/0000186.pdf?sequence=1&isAllowed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset from sklearn.\n",
    "boston_dataset = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us view what we have downloaded using the following command. \n",
    "boston_dataset\n",
    "#It displays the downloaded dataset in its raw form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us see the type of 'data'.\n",
    "type(boston_dataset)\n",
    "#It shows a special object type bunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir function gives list of all the attributes and methods for this object bunch.\n",
    "dir(boston_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us explore these attributes one by one.\n",
    "#Printing description of the dataset using the DESCR attribute.\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the 'data' attribute.\n",
    "print(boston_dataset.data)\n",
    "#These are 13 attributes stored in a ndarray (try writing command to check/verify this datatype)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the 'feature_names' attribute.\n",
    "print(boston_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the 'file_name' attribute.\n",
    "print(boston_dataset.filename)\n",
    "#This is the csv file that we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the 'target' attribute.\n",
    "print(boston_dataset.target)\n",
    "#Target is another ndarray; use type function to verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK FOR YOU\n",
    "#Print shape of the two ndarrays of features (boston_dataset.data) and target (boston_dataset.target).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us create a dataframe from the two ndarrays 'data' and 'target' from boston_dataset.\n",
    "dataset = pd.DataFrame(data=boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "#Add column with the price (target).\n",
    "dataset['PRICE'] = boston_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see the dataframe we have created.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK FOR YOU\n",
    "#Run info and describe methods on this dataframe to explore the statistics info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us normalize the PTRATIO column using the normalization formula \\discussed in class.\n",
    "dataset['PTRATIO']=(dataset['PTRATIO']-dataset['PTRATIO'].min())/(dataset['PTRATIO'].max()-dataset['PTRATIO'].min())\n",
    "#Let us check the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check the min and max value of this normalized column now.\n",
    "print('Minimum value is',dataset['PTRATIO'].min())\n",
    "print('Maximum value is',dataset['PTRATIO'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An alternate way is to use methods from sklearn package.\n",
    "#Let us use this alternate method now to normalize LSTAT column.\n",
    "\n",
    "#It is a 2-step process. We define an object of MinMaxScaler type first,\n",
    "#and then call the method fit_transform on this object to perform normalization.\n",
    "#The fit_transform method takes and ndarray of shape (n_samples, n_features).\n",
    "\n",
    "#Defining min-max scaler object\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "#Normailize data \n",
    "#dataset['LSTAT']=pd.DataFrame(minmax_scaler.fit_transform(dataset[['LSTAT']]))\n",
    "#Note that fit_transform method returns a ndarray, so we need to typecast it back to dataframe before placong it back into out dataset.\n",
    "\n",
    "#this method can be run on whole dataset at once to normalize the all the columns in one go.\n",
    "#Try it ouit if you want using the following command.\n",
    "column_headers=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','PRICE']\n",
    "dataset=pd.DataFrame(minmax_scaler.fit_transform(dataset),columns=column_headers)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check the min and max value of this normalized column now.\n",
    "print('Minimum value is',dataset['LSTAT'].min())\n",
    "print('Maximum value is',dataset['LSTAT'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data - Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us run describe method and observe standard deviation and mean of various features.\n",
    "dataset.describe()\n",
    "# see row 2 and row 3 for mean and standard deviation respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us atndardize the ZN column.\n",
    "#std and mean are the methods that can be used to find mean and standard deviation of a feature or all features.\n",
    "dataset['ZN']=(dataset['ZN']-dataset['ZN'].mean())/dataset['ZN'].std()\n",
    "#Let us check the dataset description now.\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us view the dataset too.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check the min and max value of this normalized column now.\n",
    "print('Minimum value is',dataset['ZN'].min())\n",
    "print('Maximum value is',dataset['ZN'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An alternate way is to use methods from skleyarn package.\n",
    "#Let us use this alternate method now to normalize INDUS column.\n",
    "\n",
    "#It is a 2-step process. We define an object of StandardScaler type first,\n",
    "#and then call the method fit_transform on this object to perform standardization.\n",
    "#The fit_transform method takes and ndarray of shape (n_samples, n_features).\n",
    "\n",
    "#Defining standard scaler object\n",
    "stan_scaler = StandardScaler()\n",
    "\n",
    "#Normailize data \n",
    "dataset['INDUS']=pd.DataFrame(stan_scaler.fit_transform(dataset[['INDUS']]))\n",
    "#Note that fit_transform method returns a ndarray, so we need to typecast it back to dataframe before placong it back into out dataset.\n",
    "\n",
    "#this method can be run on whole dataset at once to normalize the all the columns in one go.\n",
    "#Try it ouit if you want using the following command.\n",
    "#dataset=pd.DataFrame(stan_scaler.fit_transform(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check the dataset description now.\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check the min and max value of this normalized column now.\n",
    "print('Minimum value is',dataset['INDUS'].min())\n",
    "print('Maximum value is',dataset['INDUS'].max())\n",
    "#observe the range is pretty much similar to that of ZN that we standardized earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
